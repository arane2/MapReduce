{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting NYTimes data for topic : Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soccer\n",
      "Number of articles for Soccer obtained are : 250\n",
      "Swimming\n",
      "Number of articles for Swimming obtained are : 183\n",
      "Basketball\n",
      "Number of articles for Basketball obtained are : 250\n",
      "Golf\n",
      "Number of articles for Golf obtained are : 250\n",
      "Tennis\n",
      "Number of articles for Tennis obtained are : 250\n"
     ]
    }
   ],
   "source": [
    "from nytimesarticle import articleAPI\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "api = articleAPI(\"5zyZtAbijJdB21cuSw0UvUTUmyJfE7G8\")\n",
    "\n",
    "\n",
    "\n",
    "words = ['Soccer','Swimming','Basketball','Golf','Tennis']   #Sub-topics\n",
    "try:\n",
    "    for word in words:\n",
    "        articles = []\n",
    "        print(word)\n",
    "        for a in range(0,25):\n",
    "            time.sleep(10)\n",
    "            data = api.search(q= word, begin_date=20190101,end_date=20190414,page=a)\n",
    "            for i in range(0,len(data['response']['docs'])):\n",
    "                url = data['response']['docs'][i]['web_url']\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.content, 'html.parser')\n",
    "                soup.prettify()\n",
    "                file =open('/Users/aniket/Documents/HadoopData/NewYorkTimesData/NYT-'+word+'.txt','a+')\n",
    "                for j in range((len(soup.find_all('p')))-3):\n",
    "                    file.write(soup.find_all('p')[j].get_text())\n",
    "                \n",
    "                articles.append(url)\n",
    "        print('Number of articles for '+word+ ' obtained are :',len(articles))\n",
    "        file.close()\n",
    "\n",
    "except:\n",
    "    print(\"Number of articles only \",len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NYT-Golf.txt', 'NYT-Tennis.txt', 'NYT-Soccer.txt', 'NYT-Basketball.txt', 'NYT-Swimming.txt']\n"
     ]
    }
   ],
   "source": [
    "#Now we merge all the above text files into one big text file.\n",
    "import glob\n",
    "import os\n",
    "os.chdir(\"/Users/aniket/Documents/HadoopData/NewYorkTimesData\")\n",
    "filenames = glob.glob(\"*.txt\")\n",
    "\n",
    "with open('/Users/aniket/Documents/HadoopData/NewYorkTimesData/AllSportsNYT.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting Twitter data for topic : Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "api_key = \"TfNOdi7WVi9Et3xKXDY4q35m8\"\n",
    "api_secret = \"JMR4Cz2sZXdPjFhlZmNhd6ULJf5Vxnh6WzUNS6zVHwJcSNCN18\"\n",
    "access = \"1039892313823227906-DAVrNAPNZkX2QJOffEf6InWCCIZePL\"\n",
    "access_secret = \"QxOVJCTUwhwLf1EmanwluwdqGYfi08LlgpQA0A0znHrs4\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access, access_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "Total_count = []\n",
    "words = ['Soccer','Swimming','Basketball','Golf','Tennis']   #Sub-topics\n",
    "for word in words:\n",
    "    print(word)\n",
    "    csvFile = open('/Users/aniket/Documents/HadoopData/TwitterData/Tweet-'+word+'.csv', 'a')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    count = 0\n",
    "    for tweet in tweepy.Cursor(api.search,q= word,\n",
    "                               tweet_mode='extended',lang=\"en\",\n",
    "                               since=\"2019-01-01\").items(20000):\n",
    "        if (not tweet.retweeted) and ('RT @' not in tweet.full_text):\n",
    "            count +=1\n",
    "            print(count)\n",
    "            csvWriter.writerow([tweet.full_text.encode('utf-8')])\n",
    "    Total_count.append(count)\n",
    "    csvFile.close()\n",
    "    print('The following subtopic is done :',word)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29425\n"
     ]
    }
   ],
   "source": [
    "print(sum(Total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we merge the above csv files into one big csv file.\n",
    "import glob\n",
    "import os\n",
    "os.chdir(\"/Users/aniket/Documents/HadoopData/TwitterData\")\n",
    "fileList = glob.glob(\"*.csv\")\n",
    "combined_csv = pd.concat( [ pd.read_csv(f) for f in fileList ] ,axis = 0)\n",
    "combined_csv.to_csv( \"/Users/aniket/Documents/HadoopData/TwitterData/AllSports.csv\", index=False )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we convert the csv files to text file for the purpose of this project.\n",
    "\n",
    "with open(\"/Users/aniket/Documents/HadoopData/TwitterData/AllSports.txt\", \"w\") as my_output_file:\n",
    "    with open(\"/Users/aniket/Documents/HadoopData/TwitterData/AllSports.csv\", \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we begin cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import regex\n",
    "with open('/Users/aniket/Documents/HadoopData/TwitterData/AllSports.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "line = (re.sub('(RT )*@.+? ', '',data)) # dont want <RT and/or @user> tags\n",
    "line = (re.sub('http.+? ', '',line)) # dont want web adresses\n",
    "line = re.sub(',', ' ', line)\n",
    "line = line.lower()\n",
    "words = re.findall('([a-z]+?.{0,1}?[a-z]*?)\\W*\\s+',line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "alpha = []\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "for i in words:  \n",
    "    n = lemmatizer.lemmatize(i,pos = 'v') \n",
    "    alpha.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('would')\n",
    "stop_words.add('go')\n",
    "stop_words.add('get')\n",
    "stop_words.add('say')\n",
    "stop_words.add('make')\n",
    "stop_words.add('like')\n",
    "stop_words.add('also')\n",
    "new_alpha = []\n",
    "for i in alpha:\n",
    "    if i not in stop_words:\n",
    "        new_alpha.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_alpha = []\n",
    "for i in new_alpha:\n",
    "    if i.isalpha() == True:\n",
    "        new_new_alpha.append(i)\n",
    "\n",
    "omega = ''\n",
    "for i in new_new_alpha:\n",
    "    omega += i + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/Users/aniket/Documents/HadoopData/TwitterData/LatestTweets.txt','w')\n",
    "file.write(omega)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's start with NYT\n",
    "import re\n",
    "import regex\n",
    "with open('/Users/aniket/Documents/HadoopData/NewYorkTimesData/AllSportsNYT.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nex = word_tokenize(data)\n",
    "  \n",
    "alpha = []\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "for i in nex:  \n",
    "    n = lemmatizer.lemmatize(i,pos = 'v') \n",
    "    alpha.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = ''\n",
    "for i in alpha:\n",
    "    beta += i + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = (re.sub('(RT )*@.+? ', '',beta)) # dont want <RT and/or @user> tags\n",
    "line = (re.sub('http.+? ', '',line)) # dont want web adresses\n",
    "line = re.sub(',', ' ', line)\n",
    "line = line.lower()\n",
    "words = re.findall('([a-z]+?.{0,1}?[a-z]*?)\\W*\\s+',line) # regex to extract words (space delmit, remove special chars at end of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_words = []\n",
    "for i in words:\n",
    "    if i not in stop_words:\n",
    "        new_words.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_words = []\n",
    "for i in new_words:\n",
    "    if i.isalpha() == True:\n",
    "        new_new_words.append(i)\n",
    "\n",
    "beta = ''\n",
    "for i in new_new_words:\n",
    "    beta += i + ' '\n",
    "file = open('/Users/aniket/Documents/HadoopData/NewYorkTimesData/LatestNY.txt','w')\n",
    "file.write(beta)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33092</th>\n",
       "      <td>year</td>\n",
       "      <td>1932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20108</th>\n",
       "      <td>new</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16576</th>\n",
       "      <td>last</td>\n",
       "      <td>2034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11839</th>\n",
       "      <td>game</td>\n",
       "      <td>2035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32696</th>\n",
       "      <td>win</td>\n",
       "      <td>2047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19542</th>\n",
       "      <td>mr</td>\n",
       "      <td>2177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10983</th>\n",
       "      <td>first</td>\n",
       "      <td>2279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29986</th>\n",
       "      <td>time</td>\n",
       "      <td>2332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22513</th>\n",
       "      <td>play</td>\n",
       "      <td>2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29493</th>\n",
       "      <td>team</td>\n",
       "      <td>2577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20865</th>\n",
       "      <td>one</td>\n",
       "      <td>3437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  count\n",
       "33092   year   1932\n",
       "20108    new   1977\n",
       "16576   last   2034\n",
       "11839   game   2035\n",
       "32696    win   2047\n",
       "19542     mr   2177\n",
       "10983  first   2279\n",
       "29986   time   2332\n",
       "22513   play   2401\n",
       "29493   team   2577\n",
       "20865    one   3437"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('/Users/aniket/Documents/HadoopData/NY.txt', sep=\"\\t\", header=None)\n",
    "df1.columns = [\"word\", \"count\"]\n",
    "Top_ten_NYT = df1.sort_values(by=['count'])\n",
    "Top_ten_NYT.tail(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_csv('/Users/aniket/Documents/HadoopData/TW.txt', sep=\"\\t\", header=None)\n",
    "df2.columns = [\"word\", \"count\"]\n",
    "Top_ten_Twitter = df2.sort_values(by=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24442</th>\n",
       "      <td>tiger</td>\n",
       "      <td>1540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23883</th>\n",
       "      <td>team</td>\n",
       "      <td>1596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26306</th>\n",
       "      <td>watch</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22691</th>\n",
       "      <td>sport</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>game</td>\n",
       "      <td>2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26763</th>\n",
       "      <td>win</td>\n",
       "      <td>2799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18640</th>\n",
       "      <td>play</td>\n",
       "      <td>3040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9354</th>\n",
       "      <td>golf</td>\n",
       "      <td>4931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24022</th>\n",
       "      <td>tennis</td>\n",
       "      <td>9015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22318</th>\n",
       "      <td>soccer</td>\n",
       "      <td>11317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  count\n",
       "24442   tiger   1540\n",
       "23883    team   1596\n",
       "26306   watch   1788\n",
       "22691   sport   1968\n",
       "8755     game   2200\n",
       "26763     win   2799\n",
       "18640    play   3040\n",
       "9354     golf   4931\n",
       "24022  tennis   9015\n",
       "22318  soccer  11317"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Top_ten_Twitter.tail(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
